{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os, re, gc\n",
    "from imageio import imread\n",
    "from skimage import transform\n",
    "from vgg19 import Vgg19\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Vgg19_nts(Vgg19):\n",
    "    def __init__(self, content_image_path, style_img_path, image_size=224, ratio_content_to_style=1e-3):\n",
    "       \n",
    "        \n",
    "        self._image_size = image_size\n",
    "        self.content_img = imread(content_image_path)\n",
    "        self.content_img = transform.resize(self.content_img, [image_size, image_size])\n",
    "        self.content_img = np.expand_dims(self.content_img, axis=0)\n",
    "        \n",
    "        self.style_img = imread(style_img_path)\n",
    "        self.style_img = transform.resize(self.style_img, [image_size, image_size])\n",
    "        self.style_img = np.expand_dims(self.style_img, axis=0)\n",
    "        self._ratio_content_to_style = ratio_content_to_style\n",
    "        self._content_act_list, self._style_act_list = self.get_content_and_style_activations()\n",
    "              \n",
    "        \n",
    "        self.graph = tf.get_default_graph()\n",
    "       \n",
    "    \n",
    "    def get_content_loss(self, content, generated_act):\n",
    "        return(0.5*tf.reduce_sum(tf.square(tf.subtract(content,generated_act))))\n",
    "    \n",
    "    def get_gram(self, content):\n",
    "        content = tf.squeeze(content, axis=0)\n",
    "        gram = tf.reshape(content, (content.shape[0].value*content.shape[1].value, content.shape[2].value))\n",
    "        return(tf.matmul(tf.transpose(gram), gram))\n",
    "    \n",
    "    def get_style_El(self, content_gram, generated_gram, no_of_filters, filter_size):\n",
    "        El = tf.reduce_sum(tf.square(tf.subtract(generated_gram, content_gram)))/(4*(no_of_filters**2)*(filter_size**2))\n",
    "        return(El)\n",
    " \n",
    "    def calculate_total_loss(self):\n",
    "        loss_content = 0\n",
    "        loss_style = 0\n",
    "        for it in self._content_act_list:\n",
    "            generated_act = self.graph.get_tensor_by_name(\"{0}/{0}:0\".format(it[\"layer_name\"]))\n",
    "            content_act = tf.constant(it[\"content_act\"])\n",
    "            loss_content += it[\"w\"] * self.get_content_loss(content_act, generated_act)\n",
    "        \n",
    "        for it in self._style_act_list:\n",
    "            generated_act = self.graph.get_tensor_by_name(\"{0}/{0}:0\".format(it[\"layer_name\"]))\n",
    "            content_act = tf.constant(it[\"style_act\"])\n",
    "            \n",
    "            generated_gram = self.get_gram(generated_act)\n",
    "            content_gram = self.get_gram(content_act)\n",
    "           \n",
    "            no_of_filters = content_act.shape[3].value\n",
    "            filter_size = content_act.shape[1].value*content_act.shape[2].value\n",
    "\n",
    "            loss_style += it[\"w\"] * self.get_style_El(content_gram, generated_gram, no_of_filters, filter_size)\n",
    "            \n",
    "        return(self._ratio_content_to_style*loss_content + loss_style)\n",
    "       \n",
    "    def build_optmizer(self, learning_rate):\n",
    "        loss = self.calculate_total_loss()\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "        return([optimizer, optimizer.minimize(loss), loss])\n",
    "    \n",
    "    def get_content_and_style_activations(self):\n",
    "        \n",
    "        tf.reset_default_graph()\n",
    "        with tf.Session() as sess:\n",
    "            imgs = tf.placeholder(shape=[1, self._image_size, self._image_size, 3], dtype=tf.float32)\n",
    "            vgg19_ref = Vgg19(\"vgg19.npy\", image_size=self._image_size)\n",
    "            vgg19_ref.build(imgs)\n",
    "\n",
    "            style_acts = [{\"layer_name\":\"conv1_1\", \"style_act\":0, \"w\":1e3/64**2},\n",
    "                      {\"layer_name\":\"conv2_1\", \"style_act\":0, \"w\":1e3/128**2},\n",
    "                      {\"layer_name\":\"conv3_1\", \"style_act\":0, \"w\":1e3/256**2},\n",
    "                      {\"layer_name\":\"conv4_1\", \"style_act\":0, \"w\":1e3/512**2},\n",
    "                      {\"layer_name\":\"conv5_1\", \"style_act\":0, \"w\":1e3/512**2}]\n",
    "\n",
    "            content_acts = [{\"layer_name\":\"conv4_2\", \"content_act\":0, \"w\":1}]\n",
    "\n",
    "            for i, ls in enumerate(style_acts):\n",
    "                style_acts[i][\"style_act\"] = sess.run(\"{0}/{0}:0\".format(ls[\"layer_name\"]), \n",
    "                                                      feed_dict={imgs:self.style_img})\n",
    "            del i, ls    \n",
    "\n",
    "            for i, lc in enumerate(content_acts):\n",
    "                content_acts[i][\"content_act\"] = sess.run(\"{0}/{0}:0\".format(lc[\"layer_name\"]), \n",
    "                                                          feed_dict={imgs:self.content_img})\n",
    "        del vgg19_ref\n",
    "        gc.collect()\n",
    "      \n",
    "        return(content_acts, style_acts)\n",
    "    \n",
    "    def paint(self, learning_rate=0.005, no_of_epochs = 100, sample_rate=10, stack_len=10):\n",
    "        tf.reset_default_graph()\n",
    "        images = list()\n",
    "\n",
    "        with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "\n",
    "            imgs = tf.get_variable(shape=[1, self._image_size, self._image_size, 3], initializer=tf.random_uniform_initializer(0, 1), \n",
    "                                   name=\"painted\", dtype=tf.float32) \n",
    "\n",
    "            sess.run(tf.variables_initializer([imgs]))\n",
    "            with tf.device(\"/cpu:0\"):\n",
    "                Vgg19.__init__(self, vgg19_npy_path=\"vgg19.npy\",image_size=self._image_size)\n",
    "                Vgg19.build(self, imgs)\n",
    " \n",
    "                adam_op, op_min, loss = self.build_optmizer(learning_rate=learning_rate)\n",
    "                adam_vars = [v for v in self.graph.get_collection(\"variables\") \n",
    "                             if v.name in ['beta1_power:0', 'beta2_power:0', 'painted/Adam:0', 'painted/Adam_1:0']]\n",
    "                sess.run(tf.variables_initializer(adam_vars))\n",
    "        \n",
    "            for epoch in range(no_of_epochs):\n",
    "                _, ls = sess.run([op_min, loss])\n",
    "\n",
    "                if epoch%sample_rate==0:\n",
    "                    images.append(np.clip(sess.run(imgs), 0, 1))\n",
    "                    if len(images)>stack_len:\n",
    "                        images = images[-stack_len::]\n",
    "                print(\"\\r loss:{:<8}  epoch:{} of {}\".format(ls, epoch, no_of_epochs), end=\" \")   \n",
    "        return(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shawnr/miniconda3/envs/tensorflow/lib/python3.5/site-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "npy file loaded\n",
      "build model started\n",
      "build model finished: 0s\n"
     ]
    }
   ],
   "source": [
    "vv = Vgg19_nts(content_image_path=\"sh.jpg\", style_img_path=\"vangogh_starry_night.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "npy file loaded\n",
      "build model started\n",
      "build model finished: 0s\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Tensor(\"conv4_2/conv4_2:0\", shape=(1, 28, 28, 512), dtype=float32) must be from the same graph as Tensor(\"Const_1:0\", shape=(1, 28, 28, 512), dtype=float32, device=/device:CPU:0).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-5a5af99f3fbd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpaint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-d72f092f2966>\u001b[0m in \u001b[0;36mpaint\u001b[0;34m(self, learning_rate, no_of_epochs, sample_rate, stack_len)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mVgg19\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m                 \u001b[0madam_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_min\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_optmizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m                 adam_vars = [v for v in self.graph.get_collection(\"variables\") \n\u001b[1;32m    104\u001b[0m                              if v.name in ['beta1_power:0', 'beta2_power:0', 'painted/Adam:0', 'painted/Adam_1:0']]\n",
      "\u001b[0;32m<ipython-input-10-d72f092f2966>\u001b[0m in \u001b[0;36mbuild_optmizer\u001b[0;34m(self, learning_rate)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbuild_optmizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_total_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-d72f092f2966>\u001b[0m in \u001b[0;36mcalculate_total_loss\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mgenerated_act\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tensor_by_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{0}/{0}:0\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"layer_name\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mcontent_act\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"content_act\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0mloss_content\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"w\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_content_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_act\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerated_act\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_style_act_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-d72f092f2966>\u001b[0m in \u001b[0;36mget_content_loss\u001b[0;34m(self, content, generated_act)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_content_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerated_act\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubtract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgenerated_act\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_gram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shawnr/miniconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36msubtract\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msubtract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shawnr/miniconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36m_sub\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   4634\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_graph_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4635\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m-> 4636\u001b[0;31m         \"Sub\", x=x, y=y, name=name)\n\u001b[0m\u001b[1;32m   4637\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4638\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shawnr/miniconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    348\u001b[0m       \u001b[0;31m# Need to flatten all the arguments into a list.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m       \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m       \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_graph_from_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_Flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeywords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m       \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAssertionError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shawnr/miniconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_get_graph_from_inputs\u001b[0;34m(op_input_list, graph)\u001b[0m\n\u001b[1;32m   4634\u001b[0m         \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4635\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0moriginal_graph_element\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4636\u001b[0;31m         \u001b[0m_assert_same_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_graph_element\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4637\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4638\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s is not from the passed-in graph.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shawnr/miniconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_assert_same_graph\u001b[0;34m(original_item, item)\u001b[0m\n\u001b[1;32m   4570\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0moriginal_item\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4571\u001b[0m     raise ValueError(\"%s must be from the same graph as %s.\" % (item,\n\u001b[0;32m-> 4572\u001b[0;31m                                                                 original_item))\n\u001b[0m\u001b[1;32m   4573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Tensor(\"conv4_2/conv4_2:0\", shape=(1, 28, 28, 512), dtype=float32) must be from the same graph as Tensor(\"Const_1:0\", shape=(1, 28, 28, 512), dtype=float32, device=/device:CPU:0)."
     ]
    }
   ],
   "source": [
    "f = vv.paint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "content_img = imread(\"sh.jpg\")\n",
    "content_img = transform.resize(content_img, [224, 224])\n",
    "content_img = np.expand_dims(content_img, axis=0)\n",
    "style_img = imread(\"WorkNo217.jpg\")\n",
    "style_img = transform.resize(style_img, [224, 224])\n",
    "style_img = np.expand_dims(style_img, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "tf.reset_default_graph()\n",
    "img_init=None\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    imgs = tf.placeholder(shape=[1, 224, 224, 3], dtype=tf.float32)\n",
    "    vgg_ref = Vgg19_nts(imgs=imgs)\n",
    "\n",
    "    style_acts = [{\"layer_name\":\"conv1_1\", \"style_act\":0, \"w\":1e3/64**2},\n",
    "                  {\"layer_name\":\"conv2_1\", \"style_act\":0, \"w\":1e3/128**2},\n",
    "                  {\"layer_name\":\"conv3_1\", \"style_act\":0, \"w\":1e3/256**2},\n",
    "                  {\"layer_name\":\"conv4_1\", \"style_act\":0, \"w\":1e3/512**2},\n",
    "                  {\"layer_name\":\"conv5_1\", \"style_act\":0, \"w\":1e3/512**2}]\n",
    "    \n",
    "    content_acts = [{\"layer_name\":\"conv4_2\", \"content_act\":0, \"w\":1}]\n",
    "    \n",
    "    for i, ls in enumerate(style_acts):\n",
    "        style_acts[i][\"style_act\"] = sess.run(\"{0}/{0}:0\".format(ls[\"layer_name\"]), feed_dict={imgs:style_img})\n",
    "    del i, ls    \n",
    "    for i, lc in enumerate(content_acts):\n",
    "        content_acts[i][\"content_act\"] = sess.run(\"{0}/{0}:0\".format(lc[\"layer_name\"]), feed_dict={imgs:content_img})\n",
    "        \n",
    "del vgg_ref, imgs\n",
    "gc.collect()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "images = list()\n",
    "\n",
    "with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "    \n",
    "   \n",
    "    imgs = tf.get_variable(shape=[1, 224, 224, 3], initializer=tf.random_uniform_initializer(0, 1), \n",
    "                           name=\"painted\", dtype=tf.float32) \n",
    "\n",
    "    #imgs = tf.get_variable(initializer=img_init, name=\"painted\", dtype=tf.float32) \n",
    "    #images.append(img_init)\n",
    "    \n",
    "    sess.run(tf.variables_initializer([imgs]))\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "        vgg_paint = Vgg19_nts(imgs=imgs, ratio_content_to_style=1e-3, \n",
    "                              style_act_list=style_acts, content_act_list=content_acts)\n",
    "        lr = tf.placeholder(shape=[], dtype=tf.float32)\n",
    "        \n",
    "        adam_op, op_min, loss = vgg_paint.build_optmizer(learning_rate=lr)\n",
    "        adam_vars = [v for v in vgg_paint.graph.get_collection(\"variables\") \n",
    "                     if v.name in ['beta1_power:0', 'beta2_power:0', 'painted/Adam:0', 'painted/Adam_1:0']]\n",
    "        sess.run(tf.variables_initializer(adam_vars))\n",
    "        \n",
    "     \n",
    "    feed_dict = {lr:0.005}\n",
    "    stack_len = 50\n",
    "    for epoch in range(600):\n",
    "        \n",
    "        _, ls = sess.run([op_min, loss], feed_dict=feed_dict)\n",
    "     \n",
    "        if epoch%10==0:\n",
    "            images.append(np.clip(sess.run(imgs), 0, 1))\n",
    "            if len(images)>stack_len:\n",
    "                images = images[-stack_len::]\n",
    "                gc.collect()\n",
    "            \n",
    "        print(\"\\r loss:{:<8}  epoch:{}\".format(ls, epoch), end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Need to add the image net mean and clip for 0 to 1\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(images[-10].squeeze())\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(images[-1].squeeze())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
